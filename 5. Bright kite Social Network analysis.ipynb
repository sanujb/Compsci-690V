{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members : Anmol Singh Suag, Sanuj Bhatia\n",
    "\n",
    "We have chosen to work on <b> VAST Challenge 2015 </b> for the final project. The VAST Challenge 2015 has 2 Mini-challenges and 1 Grand Challenge. For the Homeworks 5-7, we would be working on the <b>Mini-Challenge 1</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About MC1 of Vast Challenge 2015\n",
    "\n",
    "* Mini-Challenge 1 of VAST Challenge 2015, provides location tracking information of the paying visitors of a theme park for 3 consecutive days. This data has been made possible as all paying visitors are made to use the park app while they are in the park.\n",
    "\n",
    "* The dataset includes the following columns for 3 days.\n",
    "    ID : The visitor ID of the paid park visitor\n",
    "    Timestamp : The time of activity\n",
    "    Activity Type : Check-in or Movement\n",
    "    X : The X-Coordinate of location\n",
    "    Y : The Y-Coordinate of location\n",
    "\n",
    "* Data for every visitor is provided for each second, but isn't tracked after the visitor checks into a ride at the theme work. We assume the visitor to be inside the the ride until he appears back on the location grid for movement tracking.\n",
    "\n",
    "* Users using the same app would have the same ID if they visit for more than one day.\n",
    "\n",
    "* The Mini-Challenge 1 asks us to analyse this data to identify:\n",
    "    * Number of group types that visit the park\n",
    "    * Size of these groups\n",
    "    * Most frequently visited places by these groups\n",
    "    * Observations about the groups from the data\n",
    "    * Activity patterns of the group\n",
    "    * Suggesting improvement in the park\n",
    "    * Anomalies in the park over the 3 days\n",
    "\n",
    "This is a very interesting and complex problem-set and we had not worked with spatio-temporal data-sets before. Hence, we chose this VAST Challenge to augment our understanding about the same and use Machine Learning algorithms to solve this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing commonly used ML Libraries\n",
    "from bokeh.io import output_notebook, show\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import math\n",
    "import copy\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold   #For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"fc906dc8-b9b6-4712-8e62-709c5515e562\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"fc906dc8-b9b6-4712-8e62-709c5515e562\");\n",
       "      el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"fc906dc8-b9b6-4712-8e62-709c5515e562\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'fc906dc8-b9b6-4712-8e62-709c5515e562' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"fc906dc8-b9b6-4712-8e62-709c5515e562\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"fc906dc8-b9b6-4712-8e62-709c5515e562\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting similar data-set\n",
    "\n",
    "* For Homework 5, we had to search for a data-set similar to the one provided for MC1 of VAST Challenge 2015.\n",
    "\n",
    "* After searching through myriad data-set repositories, we found 2 very similar data-sets at stanford's data repository. These are 'BrightKite' and 'Gowalla'.\n",
    "    * BrightKite Data-set Link : https://snap.stanford.edu/data/loc-brightkite.html\n",
    "    * Gowalla Data-set Link : https://snap.stanford.edu/data/loc-gowalla.html\n",
    "\n",
    "* As 'BrightKite' and 'Gowalla' have the same structure, we chose to work on 'BrightKite' as it has comparatively smaller number of rows to process for our local machines and would serve as a good enough data-set to compare with MC1 data-set. \n",
    "\n",
    "* Brightkite was once a location-based social network where users could share their location by checking-in. The data has been gathered using their public API for around 58,000 users over a period of 2 years.\n",
    "\n",
    "* The data contains over 4.5 million rows, with each row of the following format:\n",
    "    * user_ID\n",
    "    * timestamp\n",
    "    * latitude \n",
    "    * longitude \n",
    "    * location_ID\n",
    "\n",
    "* Each user has multiple check-ins at various locations identified by the latitude and longitude. For this assignment, we have taken the first 2000 users of the network (around 700k rows) as the time to run analyses on more data is too long.\n",
    "\n",
    "* Using the User_ID, timestamp, latitude and longitude, we can simulate the data-set for the Mini-Challenge 1.\n",
    "\n",
    "* 'BrightKite' data-set is different from the data-set of MC1 in the sense that it doesn't provide per second data and only provides check-in data. Our approach for the solution focuses on check-in counts and times, hence this can serve our purpose very well.\n",
    "\n",
    "* Below is a small glimpse of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17 01:48:53</td>\n",
       "      <td>39.747652</td>\n",
       "      <td>-104.992510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16 06:02:04</td>\n",
       "      <td>39.891383</td>\n",
       "      <td>-105.070814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16 03:48:54</td>\n",
       "      <td>39.891077</td>\n",
       "      <td>-105.068532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-14 18:25:51</td>\n",
       "      <td>39.750469</td>\n",
       "      <td>-104.999073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-14 00:21:47</td>\n",
       "      <td>39.752713</td>\n",
       "      <td>-104.996337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                time   latitude   longitude\n",
       "0   0 2010-10-17 01:48:53  39.747652 -104.992510\n",
       "1   0 2010-10-16 06:02:04  39.891383 -105.070814\n",
       "2   0 2010-10-16 03:48:54  39.891077 -105.068532\n",
       "3   0 2010-10-14 18:25:51  39.750469 -104.999073\n",
       "4   0 2010-10-14 00:21:47  39.752713 -104.996337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('Brightkite_totalCheckins.txt', header=None, names=['id', 'time', 'latitude', 'longitude', 'location_id'], usecols=['id', 'time', 'latitude', 'longitude'], nrows=713652, parse_dates=[1], infer_datetime_format=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "1. We round the latitude and longitude to 2 decimal places to account for slight variation in checking-in at the same location. Second decimal place comes to anywhere from 700m to 1.1km variation, depending upon the exact location. This way, we can recognise unique locations throughout the data. \n",
    "\n",
    "2. The location ID column was removed because it was a hash value combining the 6 decimal place latitude and longitudes, and is not helpful in actually recognising unique locations after rounding them off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17 01:48:53</td>\n",
       "      <td>39.75</td>\n",
       "      <td>-104.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16 06:02:04</td>\n",
       "      <td>39.89</td>\n",
       "      <td>-105.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16 03:48:54</td>\n",
       "      <td>39.89</td>\n",
       "      <td>-105.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-14 18:25:51</td>\n",
       "      <td>39.75</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-14 00:21:47</td>\n",
       "      <td>39.75</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                time  latitude  longitude\n",
       "0   0 2010-10-17 01:48:53     39.75    -104.99\n",
       "1   0 2010-10-16 06:02:04     39.89    -105.07\n",
       "2   0 2010-10-16 03:48:54     39.89    -105.07\n",
       "3   0 2010-10-14 18:25:51     39.75    -105.00\n",
       "4   0 2010-10-14 00:21:47     39.75    -105.00"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['latitude'] = round(df['latitude'], 2)\n",
    "df['longitude'] = round(df['longitude'], 2)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Approach 1\n",
    "\n",
    "* We mark unique locations as a conjugation of the truncated latitude and longitude values. These conjugated location identifiers would help us in clustering the users that have checked-in similar locations.\n",
    "\n",
    "* Then we create a 2-D matrix of user-Ids and these location identifiers. The cells in the matrix contain the number of times the user has checked-in at that specific location.\n",
    "\n",
    "* We aim to cluster the users based on this check-in matrix. Users that have similar check-ins and similar number of check-ins at different locations can be assumed to be a part of a group.\n",
    "\n",
    "* Note that we haven't considered the time of check-ins in this approach. We will use the time-stamps to validate the results of our clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_locations = {}\n",
    "id_visits = {}\n",
    "for i in range(713652):\n",
    "    loc_id = str(df['latitude'][i])+str(df['longitude'][i])\n",
    "    unique_locations[loc_id] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43028\n"
     ]
    }
   ],
   "source": [
    "print (len(unique_locations))\n",
    "unique_locations = list(unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = np.zeros(shape=(2000,43028))\n",
    "for i in range(713652):\n",
    "    userId = df['id'][i]\n",
    "    locIndex = unique_locations.index(str(df['latitude'][i])+str(df['longitude'][i]))\n",
    "    users[userId][locIndex]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithm Used\n",
    "\n",
    "* For this approach, we use KMeans Clustering algorithm. Kmeans is an unsupervised and robust learning algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n",
    "\n",
    "* The number of clusters in KMeans is fixed before-hand. After analysing the nature of data and probable number of groups that could be identified, we ran the algorithm on our user-checkin matrix with different number of cluster values. After many test runs, we finalised the cluster count value as 1000 to be a good test guess.\n",
    "\n",
    "* We would be using KMeans various times during the solution of this Mini-Challenge. In the current appraoch we use KMeans on userId and check-in count matrix, in the later approaches we would use KMeans to cluster users with similar time of check-ins at the same locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=1000, n_jobs=-1, n_init=7).fit(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Groups\n",
    "kmeans_groups = kmeans.labels_\n",
    "groups = {}\n",
    "for i in range(len(kmeans_groups)):\n",
    "    if kmeans_groups[i] in groups:\n",
    "        groups[kmeans_groups[i]].append(i)\n",
    "    else:\n",
    "        groups[kmeans_groups[i]] = [i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a dictionary of 'Cluster Label' to 'UserIds'. The users belonging to the same cluster are hoped to be a part of the same group.\n",
    "\n",
    "* We have found 20 decently sized groups, but many other clusters had a single user. This is possible because:\n",
    "    * We chose a smaller subset of the full data.\n",
    "    * The time range of check-in spans across years.\n",
    "    * This is a social-network data, users are distributed far apart\n",
    "    \n",
    "* We hope this approach to work well with the actual data-set of MC1 because that data-set is of a small location and the probability of groups to check-in at the same place is higher. Secondly, the MC1 data-set is just for 3 days as compared to BrightKite, which spans over 2 years.\n",
    "\n",
    "* We now validate one such group in BrightKite data that has been identified through KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Group 655:  9\n",
      "Members in the group (user IDs):  [82, 98, 976, 1019, 1025, 1044, 1088, 1325, 1580]\n",
      "Date: 2009-09-05, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 98]\n",
      "Date: 2008-10-25, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 1025, 1044, 1325]\n",
      "Date: 2008-10-23, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 1325, 1580]\n",
      "Date: 2008-10-21, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 1025, 1325]\n",
      "Date: 2008-09-11, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 98]\n",
      "Date: 2008-08-26, Lat: 39.74, Long: -104.98, Users Checking in together:  [82, 1088]\n",
      "Date: 2009-05-23, Lat: 40.02, Long: -105.28, Users Checking in together:  [98, 1580]\n",
      "Date: 2008-12-18, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 976, 1044]\n",
      "Date: 2008-11-26, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 976]\n",
      "Date: 2008-11-19, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 976, 1044]\n",
      "Date: 2008-11-13, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1088]\n",
      "Date: 2008-11-08, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1025]\n",
      "Date: 2008-10-22, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1325, 1580]\n",
      "Date: 2008-09-10, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1088]\n",
      "Date: 2008-08-12, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1088]\n",
      "Date: 2008-05-16, Lat: 39.74, Long: -104.98, Users Checking in together:  [98, 1580]\n",
      "Date: 2009-01-01, Lat: 39.74, Long: -104.98, Users Checking in together:  [976, 1025]\n",
      "Date: 2008-12-09, Lat: 39.74, Long: -104.98, Users Checking in together:  [976, 1044, 1088]\n",
      "Date: 2008-11-30, Lat: 39.74, Long: -104.98, Users Checking in together:  [976, 1044]\n",
      "Date: 2008-11-15, Lat: 39.74, Long: -104.98, Users Checking in together:  [976, 1044]\n",
      "Date: 2009-05-03, Lat: 39.74, Long: -104.98, Users Checking in together:  [1019, 1088]\n",
      "Date: 2008-11-10, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1044, 1088, 1325]\n",
      "Date: 2008-11-09, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1325]\n",
      "Date: 2008-11-07, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1044, 1088, 1325, 1580]\n",
      "Date: 2008-11-06, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1044]\n",
      "Date: 2008-11-04, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1088]\n",
      "Date: 2008-11-03, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1044, 1088]\n",
      "Date: 2008-11-02, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1325]\n",
      "Date: 2008-10-27, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1325]\n",
      "Date: 2008-10-26, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1325]\n",
      "Date: 2008-10-11, Lat: 39.74, Long: -104.98, Users Checking in together:  [1025, 1088]\n",
      "Date: 2008-10-24, Lat: 39.74, Long: -104.98, Users Checking in together:  [1044, 1325]\n",
      "Date: 2008-10-29, Lat: 39.74, Long: -104.98, Users Checking in together:  [1088, 1325]\n"
     ]
    }
   ],
   "source": [
    "#Looking into cluster #655\n",
    "print ('Size of Group 655: ',len(groups[655]))\n",
    "print ('Members in the group (user IDs): ', groups[655])\n",
    "\n",
    "presenceDict={}\n",
    "for i in range(len(groups[655])):\n",
    "    userId=groups[655][i]\n",
    "    userDF = df[df['id'] == userId]\n",
    "    for j in range(len(userDF)):\n",
    "        presenceID = str(userDF.iloc[j,1].date()) + '_' + str(userDF.iloc[j,2]) + '_' + str(userDF.iloc[j,3])\n",
    "        if presenceID in presenceDict:\n",
    "            if userId in presenceDict[presenceID]:\n",
    "                pass\n",
    "            else:\n",
    "                presenceDict[presenceID].append(userId)\n",
    "        else:\n",
    "            presenceDict[presenceID] = [userId]\n",
    "\n",
    "for key, value in presenceDict.items():\n",
    "    if len(value) > 1:\n",
    "        formatKeys = key.split('_')\n",
    "        print('Date: ' + formatKeys[0] + ', Lat: ' + formatKeys[1] + ', Long: ' + formatKeys[2] + ', Users Checking in together: ', value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Validation\n",
    "\n",
    "* We chose one of the clusters to validate if the userIds in that group were actually present together on the same date at the same location.\n",
    "\n",
    "* We created a dictionary where the key is a conjugation of date,latitude and logitude and the values are the userIds that were present at that date on the same location.\n",
    "\n",
    "* We have printed the results and we can see that it is actually a good group where the group members have checked-in together at the same locations on the same date!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Approaches\n",
    "* We plan to use KMeans Clustering algorithms on 2-D sparse matrices of UserIds vs Check-In times as well as UserIds vs unique spatio-temporal identifiers. We will compare the groups created in these approaches to our previous approach.\n",
    "\n",
    "* Analysing the matrices, we can identify common locations visited by groups, frequency of check-ins at various locations, group check-in behaviour over a day/week/year as well as anomalies. By plotting the check-ins over time, we can identify the dates on which more check-ins were made vs dates of low check-ins. As of now we hope to have more check-ins on weekends as compared to weekdays, but this would be clearer after making proper visualisations like:\n",
    "    * Bar Charts of frequency of check-ins over 7 day week\n",
    "    * Line graphs of check-in counts over a year\n",
    "    * Line graphs of check-in over the time of day\n",
    "\n",
    "* For the groups, we plan to plot the trajectories of the users in increasing time order and hence validate that users in the same group have common/overlapping paths. \n",
    "\n",
    "* For visualising location check-in counts, we would make scatter plots of locations with their X and Y coordiantes. The size of scatter point would be proportional to the number of check-ins it had.\n",
    "\n",
    "* By creating a histogram of group sizes vs number of groups of that size, we can identify what group sizes are more common. We expect that larger group sizes would be less common whereas smaller group sizes or individuals would be more common.\n",
    "\n",
    "* We can also determine what locations are favored over a day by finding the percentage of check-ins for a particular location. We would use interactive scatter plots to visualise the percentage of checkins a location had for that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
